#!/usr/bin/env python
# -*-python-*-

import os
import sys
import csv
import logging
import json

import mapzen.whosonfirst.utils

if __name__ == "__main__":

    # as in:
    # https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/xx67-kt59

    # which is pretty much the same as this:
    # https://data.cityofnewyork.us/Health/Restaurants-rolled-up-/59dk-tdhz

    """
    from al b. (20170723)

    ZIP code centroids should be fine.

    As mentioned, lat/lons in lieu are mainly used for blocking or grouping things
    that might be dupes together for pairwise comparison. For venues we use a geohash
    prefix of 6, which equates to about 1.2km x 0.6km, plus its 8 neighbors to avoid
    the fault line issue, so 9 tiles total covering a grid that's roughly 3.6km x 1.8km.
    If the real venue is within that distance of the postal code centroid, they'll still
    match.

    For example:
    lieu-translate-nycgov-dohmh DOHMH_New_York_City_Restaurant_Inspection_Results.csv > dohmh-venues.geojson.txt

    """

    import optparse
    opt_parser = optparse.OptionParser()

    opt_parser.add_option('-D', '--data-root', dest='data_root', action='store', default="/usr/local/data", help='... (Default is /usr/local/data')
    opt_parser.add_option('-A', '--api-key', dest='api_key', action='store', default=None, help='... (Default is None)')
    opt_parser.add_option('-O', '--outfile', dest='outfile', action='store', default=None, help="... (Default is STDOUT)")

    opt_parser.add_option('-d', '--debug', dest='debug', action='store_true', default=False, help='... (Default is False)')
    opt_parser.add_option('-v', '--verbose', dest='verbose', action='store_true', default=False, help='Be chatty (default is False)')

    options, args = opt_parser.parse_args()

    if options.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    logging.warning("EXPERIMENTAL. THIS DOESN'T REALLY DO ANYTHING YET...")

    if options.outfile:
        out = open(options.outfile, "w")
    else:
        out = sys.stdout

    orig = args[0]
    fh = open(orig, "r")

    reader = csv.DictReader(fh)
    writer = None

    postalcodes = {}
    lookup = {}

    # Please add hooks to use the API or something - we don't actually need to load 
    # the postal codes for all of the US...

    root = os.path.join(options.data_root, "whosonfirst-data-postalcode-us")
    data = os.path.join(root, "data")

    crawl = mapzen.whosonfirst.utils.crawl(data, inflate=True)

    for feature in crawl:

        props = feature["properties"]

        name = props["wof:name"]
        lat = props["geom:latitude"]
        lon = props["geom:longitude"]
        postalcodes[name] = [lat, lon]

    #

    for row in reader:

        camis = row["CAMIS"]

        if lookup.get(camis, None):
            continue

        zipcode = row["ZIPCODE"]
        coords = postalcodes.get(zipcode, None)

        if not coords:
            logging.warning("can't find coords for %s and failed, skipping %s" % (zipcode, camis))
            continue

        lat, lon = coords

        row["LATITUDE"] = lat
        row["LONGITUDE"] = lon

        """
        if not writer:
            writer = csv.DictWriter(sys.stdout, fieldnames=row.keys())
            writer.writeheader()
        """

        keys = "CAMIS,DBA,BORO,BUILDING,STREET,ZIPCODE,PHONE,CUISINE DESCRIPTION,LATITUDE,LONGITUDE"
        keys = keys.split(",")

        for k, v in row.items():

            if not k in keys:
                del(row[k])

        """
        writer.writerow(row)
        """

        id = row["CAMIS"]
        name = row["DBA"]
        
        housenumber = row["BUILDING"]
        street = row["STREET"]
        city = row["BORO"]
        region = "NY"
        postal = zipcode
        
        lat = row["LATITUDE"]
        lon = row["LONGITUDE"]

        addr_full = " ".join((housenumber, street, city, region, postal))

        source = "nyc_doh"
        source_id = "%s:%s" % (source, id)

        geom = { "type": "Point", "coordinates": [ lon, lat ] }

        props = {
            "addr:housenumber": housenumber,
            "addr:street": street,
            "addr:full": addr_full,
            "mz:is_approximate": 1
        }

        for k, v in row.items():

            k = k.lower()
            fq_k = "%s:%s" % (source, k)
            props[fq_k] = v

        feature = {
            "id": source_id,
            "geometry": geom,
            "properties": props,
        }

        out.write(json.dumps(feature))
        out.write("\n")
        
        lookup[camis] = True
    
